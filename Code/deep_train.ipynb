{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.9.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from main import backgammon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackgammonDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        # Load CSV file\n",
    "        data = pd.read_csv(file_path, header=None)  # No header in dataset\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        self.X = data.iloc[:, :-1].values.astype(np.float32)  # First 28 columns as input\n",
    "        self.y = data.iloc[:, -1].values.astype(np.float32)   # Last column as output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx]), torch.tensor(self.y[idx])\n",
    "\n",
    "# 2️⃣ Split Dataset into Training (80%) and Testing (20%)\n",
    "def get_dataloaders(file_path, batch_size=32, test_size=0.2):\n",
    "    dataset = BackgammonDataset(file_path)\n",
    "\n",
    "    # Split indices into training and testing\n",
    "    train_size = int((1 - test_size) * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackgammonNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BackgammonNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(file_path, batch_size=32, test_size=0.2):\n",
    "    dataset = BackgammonDataset(file_path)\n",
    "\n",
    "    # Split indices into training and testing\n",
    "    train_size = int((1 - test_size) * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "# 3️⃣ Define Neural Network Model\n",
    "class BackgammonNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BackgammonNet, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 4️⃣ Train the Model\n",
    "def train_model(train_loader, epochs=50, lr=0.001):\n",
    "    model = BackgammonNet()\n",
    "    criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Evaluate on test data\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        # with torch.no_grad():\n",
    "        #     for inputs, targets in test_loader:\n",
    "        #         outputs = model(inputs)\n",
    "        #         loss = criterion(outputs.squeeze(), targets)\n",
    "        #         test_loss += loss.item()\n",
    "        if epoch % 10 == 9:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {total_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "            torch.save(model.state_dict(), f\"backgammon_model_{epoch}.pth\")\n",
    "            # sleep(0.1)\n",
    "            fitness = backgammon(25, \"DEEP\",epoch)\n",
    "            print(fitness)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each point in range <= -5 - 5<=\n",
    "# White and Black bar 0, 1, 2+\n",
    "# Calculate blot exposure for each point \n",
    "# Calc % of home points occupied \n",
    "# Calc % of opp home points occupied\n",
    "# Calc % of pieces in opp home \n",
    "# Prime present ?\n",
    "# Probability blockade can be passed\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, input_file, output_file):\n",
    "        # Load data using pandas\n",
    "        self.inputs = pd.read_csv(input_file)\n",
    "        self.outputs = pd.read_csv(output_file)\n",
    "        \n",
    "        # Ensure outputs match inputs\n",
    "        assert len(self.inputs) == len(self.outputs), \"Mismatched input and output lengths\"\n",
    "        \n",
    "        # Convert to tensors\n",
    "        self.inputs = torch.tensor(self.inputs.values, dtype=torch.float32)\n",
    "        self.outputs = torch.tensor(self.outputs.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.outputs[idx]\n",
    "\n",
    "# Example usage\n",
    "def get_dataloader(input_file, output_file, batch_size=32, shuffle=True):\n",
    "    dataset = CustomDataset(input_file, output_file)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectionistModel():\n",
    "    def __init__(self):\n",
    "        super(ConnectionistModel, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(289, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, epochs=50, lr=0.001):\n",
    "    model = ConnectionistModel()\n",
    "    criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Evaluate on test data\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        # with torch.no_grad():\n",
    "        #     for inputs, targets in test_loader:\n",
    "        #         outputs = model(inputs)\n",
    "        #         loss = criterion(outputs.squeeze(), targets)\n",
    "        #         test_loss += loss.item()\n",
    "        if epoch % 10 == 9:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {total_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "            torch.save(model.state_dict(), f\"backgammon_model_{epoch}.pth\")\n",
    "            # sleep(0.1)\n",
    "            # fitness = backgammon(25, \"DEEP\",epoch)\n",
    "            # print(fitness)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ConnectionistModel' object has no attribute 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# train_loader, test_loader = get_dataloaders(file_path, batch_size=64)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# trained_model = train_model(train_loader, test_loader, epochs=100)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m get_dataloader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Data/Deep/289-input-x.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Data/Deep/289-input-y.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(trained_model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackgammon_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, epochs, lr)\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m ConnectionistModel()\n\u001b[1;32m      3\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()  \u001b[38;5;66;03m# Mean Squared Error for regression\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      7\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConnectionistModel' object has no attribute 'parameters'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = \"../Data/Deep/BoardEquity/board_equity_db.txt\"  # Update with actual dataset file\n",
    "    # train_loader, test_loader = get_dataloaders(file_path, batch_size=64)\n",
    "    # trained_model = train_model(train_loader, test_loader, epochs=100)\n",
    "    \n",
    "    train_loader = get_dataloader(\"../Data/Deep/289-input-x.txt\", \"../Data/Deep/289-input-y.txt\", batch_size=32, shuffle=True)\n",
    "    trained_model = train_model(train_loader, 50, 0.001)\n",
    "    # Save the trained model\n",
    "    torch.save(trained_model.state_dict(), \"backgammon_model.pth\")\n",
    "    print(\"Model training complete and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.3033\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.2104\n",
      "Epoch 2/50, Loss: 0.1831\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.1574\n",
      "Epoch 3/50, Loss: 0.1464\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.1346\n",
      "Epoch 4/50, Loss: 0.1289\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.1273\n",
      "Epoch 5/50, Loss: 0.1176\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.1219\n",
      "Epoch 6/50, Loss: 0.1109\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.1130\n",
      "Epoch 7/50, Loss: 0.1060\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.1095\n",
      "Epoch 8/50, Loss: 0.1023\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.1094\n",
      "Epoch 9/50, Loss: 0.0982\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.1084\n",
      "Epoch 10/50, Loss: 0.0963\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.1034\n",
      "Epoch 11/50, Loss: 0.0940\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.1014\n",
      "Epoch 12/50, Loss: 0.0911\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0998\n",
      "Epoch 13/50, Loss: 0.0896\n",
      "([0, 0, 0], 0, [0, 1, 8], 26)\n",
      "Test Loss: 0.1014\n",
      "Epoch 14/50, Loss: 0.0896\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0980\n",
      "Epoch 15/50, Loss: 0.0877\n",
      "([0, 0, 0], 0, [0, 1, 8], 26)\n",
      "Test Loss: 0.0964\n",
      "Epoch 16/50, Loss: 0.0860\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0961\n",
      "Epoch 17/50, Loss: 0.0875\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0955\n",
      "Epoch 18/50, Loss: 0.0846\n",
      "([0, 0, 0], 0, [1, 0, 8], 25)\n",
      "Test Loss: 0.0958\n",
      "Epoch 19/50, Loss: 0.0842\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0950\n",
      "Epoch 20/50, Loss: 0.0834\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0934\n",
      "Epoch 21/50, Loss: 0.0821\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0935\n",
      "Epoch 22/50, Loss: 0.0824\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0931\n",
      "Epoch 23/50, Loss: 0.0813\n",
      "([0, 0, 0], 0, [1, 0, 8], 25)\n",
      "Test Loss: 0.0978\n",
      "Epoch 24/50, Loss: 0.0812\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0921\n",
      "Epoch 25/50, Loss: 0.0806\n",
      "([0, 0, 0], 0, [0, 1, 8], 26)\n",
      "Test Loss: 0.0929\n",
      "Epoch 26/50, Loss: 0.0827\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0918\n",
      "Epoch 27/50, Loss: 0.0803\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0916\n",
      "Epoch 28/50, Loss: 0.0803\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0926\n",
      "Epoch 29/50, Loss: 0.0806\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0913\n",
      "Epoch 30/50, Loss: 0.0803\n",
      "([0, 0, 0], 0, [0, 1, 8], 26)\n",
      "Test Loss: 0.0936\n",
      "Epoch 31/50, Loss: 0.0797\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0923\n",
      "Epoch 32/50, Loss: 0.0809\n",
      "([0, 0, 0], 0, [0, 1, 8], 26)\n",
      "Test Loss: 0.0944\n",
      "Epoch 33/50, Loss: 0.0795\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0912\n",
      "Epoch 34/50, Loss: 0.0796\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0915\n",
      "Epoch 35/50, Loss: 0.0800\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0944\n",
      "Epoch 36/50, Loss: 0.0798\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0917\n",
      "Epoch 37/50, Loss: 0.0805\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0967\n",
      "Epoch 38/50, Loss: 0.0796\n",
      "([0, 0, 0], 0, [0, 1, 8], 26)\n",
      "Test Loss: 0.0911\n",
      "Epoch 39/50, Loss: 0.0793\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0909\n",
      "Epoch 40/50, Loss: 0.0790\n",
      "([0, 0, 0], 0, [0, 1, 8], 26)\n",
      "Test Loss: 0.0907\n",
      "Epoch 41/50, Loss: 0.0790\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0913\n",
      "Epoch 42/50, Loss: 0.0785\n",
      "([1, 0, 0], 1, [0, 0, 9], 27)\n",
      "Test Loss: 0.0911\n",
      "Epoch 43/50, Loss: 0.0799\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0930\n",
      "Epoch 44/50, Loss: 0.0794\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0927\n",
      "Epoch 45/50, Loss: 0.0790\n",
      "([0, 0, 0], 0, [0, 1, 8], 26)\n",
      "Test Loss: 0.0925\n",
      "Epoch 46/50, Loss: 0.0795\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0929\n",
      "Epoch 47/50, Loss: 0.0792\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0906\n",
      "Epoch 48/50, Loss: 0.0777\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0930\n",
      "Epoch 49/50, Loss: 0.0791\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0920\n",
      "Epoch 50/50, Loss: 0.0799\n",
      "([0, 0, 0], 0, [0, 0, 9], 27)\n",
      "Test Loss: 0.0930\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory /models does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 92>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m     91\u001b[0m model \u001b[38;5;241m=\u001b[39m train_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Data/Deep/289-input-x.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../Data/Deep/289-input-y.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/models/backgammon_model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel training complete and saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/torch/serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    625\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 628\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    629\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/torch/serialization.py:502\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/torch/serialization.py:473\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 473\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory /models does not exist."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_file, output_file):\n",
    "        # Load data using pandas\n",
    "        self.inputs = pd.read_csv(input_file)\n",
    "        self.outputs = pd.read_csv(output_file)\n",
    "        \n",
    "        # Ensure outputs match inputs\n",
    "        assert len(self.inputs) == len(self.outputs), \"Mismatched input and output lengths\"\n",
    "        \n",
    "        # Convert to tensors\n",
    "        self.inputs = torch.tensor(self.inputs.values, dtype=torch.float32)\n",
    "        self.outputs = torch.tensor(self.outputs.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.outputs[idx]\n",
    "\n",
    "# Example usage\n",
    "def get_dataloaders(input_file, output_file, batch_size=32, shuffle=True):\n",
    "    dataset = CustomDataset(input_file, output_file)\n",
    "    \n",
    "    # Split dataset into 80% training and 20% test set\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Define the model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(289, 1)\n",
    "        # self.relu = nn.ReLU()\n",
    "        # self.fc2 = nn.Linear(12, 12)\n",
    "        # self.fc3 = nn.Linear(12, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.fc2(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Training loop\n",
    "def train_model(input_file, output_file, epochs=50, batch_size=32, learning_rate=0.002):\n",
    "    train_loader, test_loader = get_dataloaders(input_file, output_file, batch_size)\n",
    "    model = SimpleModel()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item()\n",
    "                \n",
    "            torch.save(model.state_dict(), f\"backgammon_model_{epoch}.pth\")\n",
    "            fitness = backgammon(25, \"DEEP\",epoch)\n",
    "            print(fitness)\n",
    "        print(f\"Test Loss: {test_loss/len(test_loader):.4f}\")\n",
    "        \n",
    "    return model\n",
    "# Example usage:\n",
    "model = train_model(\"../Data/Deep/289-input-x.txt\", \"../Data/Deep/289-input-y.txt\")\n",
    "torch.save(model.state_dict(), \"backgammon_model.pth\")\n",
    "print(\"Model training complete and saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
